<!DOCTYPE HTML>
<html lang="en-gb" class="no-js">
<head>
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
<meta name="viewport" content="width=device-width,initial-scale=1.0,maximum-scale=2.5,user-scalable=yes">
    <meta name="citation_publisher" content="Springer, Cham"/>
    <meta name="citation_title" content="Person Re-identification: Current Approaches and Future Challenges"/>
    <meta name="citation_doi" content="10.1007/978-3-030-03243-2_825-1"/>
    <meta name="citation_language" content="en"/>
    <meta name="citation_abstract_html_url" content="https://link.springer.com/referenceworkentry/10.1007/978-3-030-03243-2_825-1"/>
    <meta name="citation_fulltext_html_url" content="https://link.springer.com/referenceworkentry/10.1007/978-3-030-03243-2_825-1"/>
    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007%2F978-3-030-03243-2_825-1.pdf"/>
    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/978-3-030-03243-2_825-1&amp;api_key="/>
    <meta name="citation_firstpage" content="1"/>
    <meta name="citation_lastpage" content="5"/>
    <meta name="citation_author" content="Rameswar Panda"/>
    <meta name="citation_author_institution" content="University of California"/>
    <meta name="citation_author" content="Amit K. Roy-Chowdhury"/>
    <meta name="citation_author_institution" content="University of California"/>
    <meta name="dc.identifier" content="10.1007/978-3-030-03243-2_825-1"/>
    <meta name="format-detection" content="telephone=no"/>
    <meta name="description" content="Multi-camera scene understanding; Multi-camera tracking; Person Re-identification Calibration of Multi-Camera Setups Multi-Camera Human Action Recognition Tracking in Camera Networks Transfer Learning"/>
    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:title" content="Person Re-identification: Current Approaches and Future Challenges"/>
    <meta name="twitter:image" content="https://static-content.springer.com/cover/book/978-3-030-03243-2.jpg"/>
    <meta name="twitter:image:alt" content="Content cover image"/>
    <meta name="twitter:site" content="SpringerLink"/>
    <meta name="twitter:description" content="Multi-camera scene understanding; Multi-camera tracking; Person Re-identification Calibration of Multi-Camera Setups Multi-Camera Human Action Recognition Tracking in Camera Networks Transfer Learning"/>
    <meta name="citation_inbook_title" content="Computer Vision"/>
    <meta name="citation_publication_date" content="2020"/>
    <meta property="og:title" content="Person Re-identification: Current Approaches and Future Challenges"/>
    <meta property="og:type" content="Paper"/>
    <meta property="og:url" content="https://link.springer.com/referenceworkentry/10.1007/978-3-030-03243-2_825-1"/>
    <meta property="og:image" content="https://static-content.springer.com/cover/book/978-3-030-03243-2.jpg"/>
    <meta property="og:site_name" content="SpringerLink"/>
    <meta property="og:description" content="Multi-camera scene understanding; Multi-camera tracking; Person Re-identification Calibration of Multi-Camera Setups Multi-Camera Human Action Recognition Tracking in Camera Networks Transfer Learning"/>

    <title>Person Re-identification: Current Approaches and Future Challenges | SpringerLink</title>
    <link rel="canonical" href="https://link.springer.com/referenceworkentry/10.1007/978-3-030-03243-2_825-1"/>
    <link rel="shortcut icon" href="/springerlink-static/632953562/images/favicon/favicon.ico">
<link rel="icon" sizes="16x16 32x32 48x48" href="/springerlink-static/632953562/images/favicon/favicon.ico">
<link rel="icon" sizes="16x16" type="image/png" href="/springerlink-static/632953562/images/favicon/favicon-16x16.png">
<link rel="icon" sizes="32x32" type="image/png" href="/springerlink-static/632953562/images/favicon/favicon-32x32.png">
<link rel="icon" sizes="48x48" type="image/png" href="/springerlink-static/632953562/images/favicon/favicon-48x48.png">
<link rel="apple-touch-icon" href="/springerlink-static/632953562/images/favicon/app-icon-iphone@3x.png">
<link rel="apple-touch-icon" sizes="72x72" href="/springerlink-static/632953562/images/favicon/ic_launcher_hdpi.png">
<link rel="apple-touch-icon" sizes="76x76" href="/springerlink-static/632953562/images/favicon/app-icon-ipad.png">
<link rel="apple-touch-icon" sizes="114x114" href="/springerlink-static/632953562/images/favicon/app-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="/springerlink-static/632953562/images/favicon/app-icon-iphone@2x.png">
<link rel="apple-touch-icon" sizes="144x144" href="/springerlink-static/632953562/images/favicon/ic_launcher_xxhdpi.png">
<link rel="apple-touch-icon" sizes="152x152" href="/springerlink-static/632953562/images/favicon/app-icon-ipad@2x.png">
<link rel="apple-touch-icon" sizes="180x180" href="/springerlink-static/632953562/images/favicon/app-icon-iphone@3x.png">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="/springerlink-static/632953562/images/favicon/ic_launcher_xxhdpi.png">
    <link rel="dns-prefetch" href="//fonts.gstatic.com">
<link rel="dns-prefetch" href="//fonts.googleapis.com">
<link rel="dns-prefetch" href="//google-analytics.com">
<link rel="dns-prefetch" href="//www.google-analytics.com">
<link rel="dns-prefetch" href="//www.googletagservices.com">
<link rel="dns-prefetch" href="//www.googletagmanager.com">
<link rel="dns-prefetch" href="//static-content.springer.com">
    <link rel="stylesheet" href="/springerlink-static/632953562/css/basic.css" media="screen">
<link rel="stylesheet" href="/springerlink-static/632953562/css/styles.css" class="js-ctm" media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
<link rel="stylesheet" href="/springerlink-static/632953562/css/print.css" media="print">


        <script type="text/javascript">
        window.Krux||((Krux=function(){Krux.q.push(arguments);}).q=[]);
        var dataLayer = [{
                'GA Key':"UA-26408784-1",
                'Features':["leaderboardadverts","eventtracker"],
                'Event Category':"Reference work entry",
                'Open Access':"N",
                'Labs':"Y",
                'DOI':"10.1007/978-3-030-03243-2_825-1",
                'VG Wort Identifier':"pw-vgzm.415900-10.1007-978-3-030-03243-2_825-1",
                'hasAccess':"Y",
                'Full HTML':"Y",
                'Has Body':"Y",
                'Static Hash':"632953562",
                'Has Preview':"N",
                'user':{"license":{"businessPartnerID":["3000481153"],"businessPartnerIDString":"3000481153"}},
                'content':{"serial":{"eissn":"","pissn":""},"book":{"seriesTitle":"","eisbn":"978-3-030-03243-2","pisbn":"978-3-030-03243-2","bookProductType":"Encyclop(a)edia","seriesId":"","title":"Computer Vision","doi":"10.1007/978-3-030-03243-2"},"attributes":{"deliveryPlatform":"bunsen"},"chapter":{"doi":"10.1007/978-3-030-03243-2_825-1"},"version":"live","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Imaging, Vision, Pattern Recognition and Graphics","2":"Artificial Intelligence","3":"Signal, Image and Speech Processing"},"secondarySubjectCodes":{"1":"I22005","2":"I21000","3":"T24051"}},"sucode":"SUCO11645"},"type":"ReferenceWorkEntry"},
                'Access Type':"subscription",
                'Page':"entry",
                'Bpids':"3000481153",
                'Bpnames':"SpringerReference Community",
                'SubjectCodes':"SCI, SCI22005, SCI21000, SCT24051",
                'session':{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},
                'eventTrackerBaseUrl':"https://event-tracker.springernature.com",
                'Country':"US",


                'CurrentPage':1,
                'MaxPage':2
        }];
    </script>

<script type="text/javascript" src="/springerlink-static/632953562/js/jquery-3.3.1.min.js"></script>

        <script type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>

<script type="text/javascript">
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>

    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
                new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
            'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-WCF9Z9');</script>

</head>
<body class="page-wrapper--flex">
        <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
                      height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>

    <div class="skip-to">
    <a class="skip-to__link pseudo-focus" href="#main-content">Skip to main content</a>
        <a href="#toc" class="skip-to__link skip-to__link--contents pseudo-focus">Skip to table of contents</a>
</div>

    <div class="page-wrapper page-wrapper--flex">
        <noscript>
    <div class="nojs-banner u-interface">
        <p>This service is more advanced with JavaScript available, learn more at <a
                href="http://activatejavascript.org" target="_blank" rel="noopener">http://activatejavascript.org</a>
        </p>
    </div>
</noscript>

        <div class="header header--flex">
                <header id="header" class="header u-interface" role="banner">
        <div class="header__content header__content--flex">
            <div class="header__menu-container">
                    <a id="logo" class="site-logo" href="/" title="Go to homepage">
                <div class="u-screenreader-only">SpringerLink</div>
    <svg class="site-logo__springer" width="148" height="30" role="img" focusable="false" aria-hidden="true">
        <image width="148" height="30" alt="" src="/springerlink-static/632953562/images/png/springerlink.png" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="/springerlink-static/632953562/images/svg/springerlink.svg"></image>
    </svg>

    </a>


                    <nav id="search-container" class="u-inline-block">
                        <div class="search">
                            <div class="search__content">
                                <form class="u-form-single-input" action="/search" method="get" role="search">
    <label for="search-springerlink">Search SpringerLink</label>
    <div class="u-relative">
        <input id="search-springerlink" name="query" type="text" autocomplete="off" value="">
        <input class="u-hide-text" type="submit" value="Submit" title="Submit">
        <svg class="u-vertical-align-absolute" width="13" height="13" viewBox="222 151 13 13" version="1.1" xmlns="http://www.w3.org/2000/svg" focusable="false" aria-hidden="true" role="presentation">
            <path d="M227 159C228.7 159 230 157.7 230 156 230 154.3 228.7 153 227 153 225.3 153 224 154.3 224 156 224 157.7 225.3 159 227 159L227 159 227 159 227 159ZM230 160.1L231.1 159 233.9 161.7C234.2 162.1 234.2 162.6 233.9 162.9 233.6 163.2 233.1 163.2 232.7 162.9L230 160.1 230 160.1 230 160.1 230 160.1ZM227 161L227 161C224.2 161 222 158.8 222 156 222 153.2 224.2 151 227 151 229.8 151 232 153.2 232 156 232 158.8 229.8 161 227 161L227 161 227 161 227 161 227 161Z" stroke="none" fill-rule="evenodd"/>
        </svg>
    </div>
</form>
                            </div>
                        </div>
                    </nav>

                    <nav class="nav-container u-interface">
    <div class="global-nav__wrapper">
        <div class="search-button">
            <a class="search-button__label" href="#search-container">
                <span class="search-button__title">Search</span><svg width="12" height="12" viewBox="222 151 12 12" version="1.1" xmlns="http://www.w3.org/2000/svg" focusable="false" aria-hidden="true" role="presentation">
                    <path d="M227 159C228.7 159 230 157.7 230 156 230 154.3 228.7 153 227 153 225.3 153 224 154.3 224 156 224 157.7 225.3 159 227 159L227 159 227 159 227 159ZM230 160.1L231.1 159 233.9 161.7C234.2 162.1 234.2 162.6 233.9 162.9 233.6 163.2 233.1 163.2 232.7 162.9L230 160.1 230 160.1 230 160.1 230 160.1ZM227 161L227 161C224.2 161 222 158.8 222 156 222 153.2 224.2 151 227 151 229.8 151 232 153.2 232 156 232 158.8 229.8 161 227 161L227 161 227 161 227 161 227 161Z" stroke="none" fill-rule="evenodd"></path>
                </svg>
            </a>
        </div>

        <ul class="global-nav" data-component="SV.Menu" data-title="Navigation menu" data-text="Menu">
            <li>
                <a href="/">
                    <span class="u-overflow-ellipsis">Home</span>
                </a>
            </li>

                <li class="global-nav__logged-out">
                    <a class="test-login-link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Freferenceworkentry%2F10.1007%252F978-3-030-03243-2_825-1">
                        <span class="u-overflow-ellipsis">Log in</span>
                    </a>
                </li>

        </ul>
    </div> 
</nav> 
            </div>

        </div>
    </header>

        </div>

        <main id="main-content" class="main-wrapper main-wrapper--flex" tabindex="-1">

            <div class="unified-header">
    <div class="unified-header__content" >
        <div class="unified-header__container">
            <h1 class="unified-header__title"><a class="unified-header__link" href="/referencework/10.1007/978-3-030-03243-2">Computer Vision</a></h1>
                <div id="edition-details" class="edition">
        <span id="test-edition-header" class="u-strong unified-header__editors">Living Edition</span>
    </div>

        </div>
    </div>
</div>

            <div id="c-tabs" class="c-tabs c-tabs--rwe u-js-hide">
                <ul class="c-tabs__list c-tabs--rwe__list" role="tablist">
    <li class="c-tabs__list-item c-tabs--rwe__list-item">
        <a href="#toc" class="c-tabs__list-link c-tabs--rwe__list-link" data-test="toc-link" title="Table of contents" role="tab" aria-controls="toc" id="tab-controls__toc">
            <svg width="24" height="24" xmlns="http://www.w3.org/2000/svg">
                <path d="M5 5v12.2676C5.2942 17.0974 5.6357 17 6 17h14c0 .5523-.4477 1-1 1H6c-.5523 0-1 .4477-1 1s.4477 1 1 1h13.5c.276 0 .5.224.5.5s-.224.5-.5.5H6c-1.1046 0-2-.8954-2-2V5c0-1.1046.8954-2 2-2h13c.5523 0 1 .4477 1 1v13h-1V4H6c-.5523 0-1 .4477-1 1zm5 1h7v4h-7V6zm1 3h5V7h-5v2zM7 5h1v12H7V5z" fill="#004AA7"/>
            </svg>
            <span aria-hidden="true">Contents</span>
        </a>
    </li>

    <li class="c-tabs__list-item c-tabs--rwe__list-item">
        <a href="#springerlink-search" class="c-tabs__list-link c-tabs--rwe__list-link" title="Search in this book" role="tab" aria-controls="springerlink-search" id="tab-controls__springerlink-search">
            <svg width="24" height="24" xmlns="http://www.w3.org/2000/svg">
                <path d="M19.4804 18.4546c.283.2828.29.7492.0067 1.0325-.275.275-.7453.2806-1.0322-.0064l-3.0443-3.0442C14.2065 17.414 12.6717 18 11 18c-3.866 0-7-3.134-7-7s3.134-7 7-7 7 3.134 7 7c0 1.6717-.586 3.2066-1.5638 4.4103l3.0442 3.0443zM11 17c3.3137 0 6-2.6863 6-6s-2.6863-6-6-6-6 2.6863-6 6 2.6863 6 6 6z" fill="#004AA7"/>
            </svg>
            <span aria-hidden="true">Search</span>
        </a>
    </li>

</ul>
            </div>

            <div id="main-container" class="main-container main-container--flex uptodate-recommendations-off">

                <div class="main-body main-body--flex" data-role="NavigationContainer">
                    <article class="main-body__content">
                            <div xmlns="http://www.w3.org/1999/xhtml" class="FulltextWrapper"><div class="ArticleHeader main-context"><div class="MainTitleSection"><h1 class="ChapterTitle" lang="en">Person Re-identification: Current Approaches and Future Challenges</h1></div><div class="authors u-clearfix" data-component="SpringerLink.Authors"><ul class="u-interface u-inline-list authors__title" data-role="AuthorsNavigation"><li><span>Authors</span></li><li><a href="#authorsandaffiliations" data-track="click" data-track-action="Authors and affiliations tab" data-track-label="">Authors and affiliations</a></li></ul><div class="authors__list" data-role="AuthorsList"><ul class="test-contributor-names"><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors__name">Rameswar Panda</span></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors__name">Amit K. Roy-Chowdhury</span></li></ul></div></div><div class="main-context__container" data-component="SpringerLink.ArticleMetrics"><div class="main-context__column"><span><span class="test-render-category">Living reference work entry</span></span><div class="article-dates"><span class="article-dates__label">First Online: </span><span class="article-dates__first-online"><time datetime="2020-01-10">10 January 2020</time></span></div><div><strong>DOI: </strong><span><span>https://doi.org/</span><span class="ChapterDOI">10.1007/978-3-030-03243-2_825-1</span></span></div></div><div class="main-context__column">    <ul id="book-metrics" class="article-metrics u-sansSerif">
            <li class="article-metrics__item">
                     <span class="test-metric-count article-metrics__views">2</span>
                     <span class="test-metric-name article-metrics__label">Downloads</span>
            </li>
    </ul>
</div></div></div><div class="cta-button-container cta-button-container--inline cta-button-container--stacked u-mb-36 test-pdf-link"><div class="cta-button-container__item">    <a href="/content/pdf/10.1007%2F978-3-030-03243-2_825-1.pdf" target="_blank" class="c-button c-button--blue c-button__icon-right test-rwepdf-link" title="Download this book in PDF format" rel="noopener" data-track="click" data-track-action="Pdf download" data-track-label="">
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" version="1.1"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g fill="#fff"><g transform="translate(12.000000, 5.000000)"><path d="M7 7.3L7 1C7 0.4 6.6 0 6 0 5.4 0 5 0.4 5 1L5 7.3 3.5 5.7C3.1 5.3 2.5 5.3 2.1 5.7L2.1 5.7C1.7 6.1 1.7 6.7 2.1 7.1L5.3 10.3C5.7 10.7 6.3 10.7 6.7 10.3L9.9 7.1C10.3 6.7 10.3 6.1 9.9 5.7L9.9 5.7C9.5 5.3 8.9 5.3 8.5 5.7L7 7.3 7 7.3ZM0 13C0 12.4 0.5 12 1 12L11 12C11.6 12 12 12.4 12 13 12 13.6 11.5 14 11 14L1 14C0.4 14 0 13.6 0 13L0 13Z"></path></g></g></g></svg>
        <span class="hide-text-small">Download</span>
        <span class="hide-text-small">entry</span>
        <span>PDF</span>
    </a>
</div><div class="cta-button-container__item cta-button-container__item--width-auto-small"><a class="c-button" href="#howtocite">How to cite</a></div></div><div id="body"><section id="Sec1" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading">Synonyms</h2><div class="content"><p id="Par1" class="Para"><span class="ExternalRef"><a href="http://link.springer.com/search?facet-eisbn=978-3-030-03243-2&amp;facet-content-type=ReferenceWorkEntry&amp;query=%20Multi-camera%20scene%20understanding%20"><span class="RefSource">Multi-camera scene understanding</span></a></span>; <span class="ExternalRef"><a href="http://link.springer.com/search?facet-eisbn=978-3-030-03243-2&amp;facet-content-type=ReferenceWorkEntry&amp;query=%20Multi-camera%20tracking%20"><span class="RefSource">Multi-camera tracking</span></a></span>; <span class="ExternalRef"><a href="http://link.springer.com/search?facet-eisbn=978-3-030-03243-2&amp;facet-content-type=ReferenceWorkEntry&amp;query=%20Person%20%20Re-identification%20"><span class="RefSource">Person Re-identification</span></a></span></p></div></section><section id="Sec2" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading">Related Concepts</h2><div class="content"><div id="Par2" class="Para"> <div class="UnorderedList"><ul class="UnorderedListMarkNone"><li> <p id="Par3" class="Para"> <span class="ExternalRef"><a href="http://link.springer.com/search?facet-eisbn=978-3-030-03243-2&amp;facet-content-type=ReferenceWorkEntry&amp;query=%20Calibration%20of%20Multi-Camera%20Setups%20"><span class="RefSource">Calibration of Multi-Camera Setups</span></a></span> </p> </li><li> <p id="Par4" class="Para"> <span class="ExternalRef"><a href="http://link.springer.com/search?facet-eisbn=978-3-030-03243-2&amp;facet-content-type=ReferenceWorkEntry&amp;query=%20Multi-Camera%20Human%20Action%20Recognition%20"><span class="RefSource">Multi-Camera Human Action Recognition</span></a></span> </p> </li><li> <p id="Par5" class="Para"> <span class="ExternalRef"><a href="http://link.springer.com/search?facet-eisbn=978-3-030-03243-2&amp;facet-content-type=ReferenceWorkEntry&amp;query=%20Tracking%20in%20Camera%20Networks%20"><span class="RefSource">Tracking in Camera Networks</span></a></span> </p> </li><li> <p id="Par6" class="Para"> <span class="ExternalRef"><a href="http://link.springer.com/search?facet-eisbn=978-3-030-03243-2&amp;facet-content-type=ReferenceWorkEntry&amp;query=%20Transfer%20Learning%20"><span class="RefSource">Transfer Learning</span></a></span> </p> </li></ul></div> </div></div></section><section id="Sec3" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading">Definition</h2><div class="content"><p id="Par7" class="Para">The objective of person re-identification (re-id) is to associate targets across cameras with non-overlapping fields of view. Specifically, a person re-identification algorithm takes two images from two non-overlapping cameras and provides a decision whether those two images are of the same person or not.</p></div></section><section id="Sec4" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading">Background</h2><div class="content"><p id="Par8" class="Para">In the last few years, the problem of re-identifying persons across multiple non-overlapping cameras has received increasing attention. A thorough recent survey on person re-identification (re-id) can be found at [<span class="CitationRef"><a href="#CR1">1</a></span>]. Most existing person re-id techniques are based on supervised learning. These methods either seek the best feature representation [<span class="CitationRef"><a href="#CR2">2</a></span>, <span class="CitationRef"><a href="#CR3">3</a></span>] or learn discriminant metrics [<span class="CitationRef"><a href="#CR4">4</a></span>, <span class="CitationRef"><a href="#CR5">5</a></span>] that yield an optimal matching score between two cameras or between a gallery and a probe image. Recently, deep learning methods have shown significant performance improvement on image classification and have been applied to person re-id [<span class="CitationRef"><a href="#CR6">6</a></span>, <span class="CitationRef"><a href="#CR7">7</a></span>]. Considering that a modest-sized camera network can easily have hundreds of cameras, these supervised re-id models will require huge amount of labeled data which are difficult to collect in real-world settings. In an effort to bypass tedious labeling of training data in supervised re-id models, there has been recent interest in using active learning for labeling examples in an interactive manner. In [<span class="CitationRef"><a href="#CR8">8</a></span>], an entropy-based selection approach is proposed for reducing manual annotation. In [<span class="CitationRef"><a href="#CR9">9</a></span>], the authors uses a dominant clustering-based approach for probe relevant set selection and utilize it for pair selection in a dynamic setting.</p><p id="Par9" class="Para">Unsupervised learning has received little attention in person re-identification because of their weak performance on benchmarking datasets compared to supervised methods. Representative methods along this direction use either hand-crafted appearance features [<span class="CitationRef"><a href="#CR10">10</a></span>] or saliency statistics [<span class="CitationRef"><a href="#CR11">11</a></span>] for matching persons without requiring huge amount of labeled data. Recently, sparse dictionary learning-based methods have also been utilized in an unsupervised setting [<span class="CitationRef"><a href="#CR12">12</a></span>].</p><p id="Par10" class="Para">Domain adaptation [<span class="CitationRef"><a href="#CR13">13</a></span>, <span class="CitationRef"><a href="#CR14">14</a></span>], which aims to adapt a source domain to a target domain, has been successfully used in many areas of computer vision and machine learning, e.g., object classification, and action recognition and speech processing. Despite its applicability in classical computer vision tasks, domain adaptation for person re-identification still remains as a challenging and under-addressed problem. Only very recently, domain adaptation for re-id has begun to be considered [<span class="CitationRef"><a href="#CR15">15</a></span>]. However, these studies consider only improving the re-id performance in a static camera network with fixed number of cameras. Furthermore, most of these approaches learn supervised models using labeled data from the target domain.</p></div></section><section id="Sec5" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading">Main Text</h2><div class="content"><p id="Par11" class="Para">With the advancement of imaging sensor technology, surveillance systems have seen remarkable increase in various applications ranging from law enforcement to large retail applications, from facility access and environment monitoring. Even though the sensing devices are becoming cheaper, monitoring a wide area by deploying a large number of cameras is still not feasible due to the amount of human supervision, privacy concerns, and maintenance costs involved. As a result, only a small part of the whole area is covered by a number of cameras with non-overlapping fields of view (FoVs). The non-overlapping camera FoVs leave blind gaps which are critical in the sense that no information can be obtained from these areas. This raises the need for automated methods able to extract and access high-level semantic information carried by the extremely high volume of recorded video data. As a result of losing a person when he/she leaves a camera FoV, it is extremely challenging to reassociate the same person at a different location and time among multiple persons. This inter-camera person association problem is known as the person re-identification problem.</p><p id="Par12" class="Para">In spite of a surge of effort put in by the research community in recent years, re-identification has remained quite an open issue due to a number of hard challenges. First, footages are recorded in an uncontrolled environment by cameras with large FoVs, generating low-resolution images of the targets. This makes the acquisition of discriminating biometric features (e.g., face and gait features) hard as well as unreliable. Due to the poor quality of the acquired biometric features, methods relying on such features perform unsatisfactorily. As a result, visual appearance features are, still, the first choice in re-identification problems. As a target’s appearance often undergoes large variations across non-overlapping camera views due to significant changes in viewing angle, lighting, background clutter, and occlusion, the appearance features for the target can be very different from camera to camera.</p><p id="Par13" class="Para">The computer vision community has tried to address the person re-identification problem by designing discriminative signatures for each target or by finding a non-Euclidean metric which minimizes the distance between features of the same target across cameras. Similar to the most other visual recognition problems, the most successful approaches have been based on supervised training phases. Labeled data across pairs of cameras are used to learn models that define the transformation between the views in two cameras, and these learned models are used to associate between images during the testing phase. However, this level of supervision hampers scalability of the problem because of the need to label quantities of data, which grows with the size of the camera network and the variety of conditions that may be encountered.</p><p id="Par14" class="Para">Below we discuss future research directions in person re-identification, especially the possibility of significantly reducing the level of supervision without any sacrifice in performance. This will ensure that it is possible to scale person re-identification problems to larger and larger networks of cameras without compromising accuracy of the association task. We specifically focus on two problems. The first problem relates to scalability of person re-identification as the number of people grows. The second relates to scalability as the size of the camera network grows.</p></div></section><section id="Sec6" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading">Optimal Subset Selection for Labeling</h2><div class="content"><p id="Par15" class="Para">Given unlabeled training data across a network of cameras and a similarity measure, can we select a minimal subset of images that should be labeled and from which the person re-identification models can be learned? The intuition here is that if we choose this minimal subset judiciously, the labels can be propagated using the similarity measure to the rest of the dataset. Thus, most of the labels would be obtained automatically with only a small subset of images being labeled.</p><p id="Par16" class="Para">Building on preliminary result on network consistent re-id [<span class="CitationRef"><a href="#CR16">16</a></span>, <span class="CitationRef"><a href="#CR17">17</a></span>], we now ask whether <em class="EmphasisTypeItalic ">consistency can be used to reduce the labeling effort</em>. However, in order to take advantage of consistency relations for reducing the labeling effort, we have to choose image pairs in a judicious manner. Toward this objective, we can represent a camera network as an edge-weighted k-partite graph. Nodes on the graph are observations of the targets, and edge weights are computed based on similarities in the observations. We formulate the pair subset selection as the following combinatorial optimization problem: given a complete <em class="EmphasisTypeItalic ">k</em>-partite graph <em class="EmphasisTypeItalic ">G</em><sub><em class="EmphasisTypeItalic ">k</em></sub> = (<em class="EmphasisTypeItalic ">V</em>, <em class="EmphasisTypeItalic ">E</em>) with nonnegative edge weights and an integer <em class="EmphasisTypeItalic ">B</em>, choose a maximum-weight set <em class="EmphasisTypeItalic ">S</em> of edges from <em class="EmphasisTypeItalic ">E</em> such that <em class="EmphasisTypeItalic ">G′</em> = (<em class="EmphasisTypeItalic ">V</em>, <em class="EmphasisTypeItalic ">S</em>) is triangle-free and |<em class="EmphasisTypeItalic ">S</em>|≤ <em class="EmphasisTypeItalic ">B</em>, where <em class="EmphasisTypeItalic ">B</em> is labeling budget. Once the subset for labeling is selected, the remaining labels can be obtained by label propagation on the graph, and existing methods for learning re-identification models can be used. Note that this subset selection is a NP-hard problem as it requires searching over every subset of image pairs. Polynomial time sub-optimal algorithms with linear or quasilinear time complexity can be adopted to solve this problem in an efficient way. Preliminary experiments in [<span class="CitationRef"><a href="#CR18">18</a></span>] have shown that we are able to achieve same recognition performance as the state of the art, with only 8% manual labels on the challenging Market-1501 dataset with six fixed cameras [<span class="CitationRef"><a href="#CR19">19</a></span>].</p><section id="Sec7" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading">On-Boarding New Cameras Through Transfer Learning</h3><p id="Par17" class="Para">We now address a very practical problem in camera networks, which has received little attention in the person re-identification literature. <em class="EmphasisTypeItalic ">Given a camera network where the inter-camera transformations/distance metrics have been learned in an intensive training phase, how can we on-board new cameras into the installed system with minimal additional effort?</em> To illustrate such a problem, let us consider a scenario with <span class="InlineEquation" id="IEq1">\(\mathcal {N}\)</span> cameras for which we have learned the optimal pair-wise distance metrics, so providing high re-id accuracy for all camera pairs. However, during a particular event, a new camera may be temporarily on-boarded to cover a certain related area that is not well-covered by the existing network of <span class="InlineEquation" id="IEq2">\(\mathcal {N}\)</span> cameras. Despite the dynamic and open nature of the world, almost all work in re-identification assume a <em class="EmphasisTypeItalic ">static</em> and <em class="EmphasisTypeItalic ">closed</em> world model of the re-id problem where the number of cameras is fixed in a network. Given newly introduced camera(s), traditional re-id methods will try to relearn the inter-camera transformations/distance metrics using a costly training phase. This is impractical since labeling data in the new camera and then learning transformations with the others is time-consuming and defeats the entire purpose of temporarily introducing the additional camera.</p><p id="Par18" class="Para">In [<span class="CitationRef"><a href="#CR20">20</a></span>], the authors have shown that it is possible to add a new camera to an existing network using transfer learning. First, they propose an unsupervised method based on geodesic flow kernel that can effectively find the best source camera to adapt with a target camera. Given camera pairs, each consisting of 1 (out of <span class="InlineEquation" id="IEq3">\(\mathcal {N}\)</span>) source camera and a target camera, they first compute a kernel over the subspaces representing the data of both cameras and then use it to find the kernel distance across the source and target camera. Then, they rank the source cameras based on the average distance and choose the one with lowest distance as the best source camera to pair with the target camera. This is intuitive since a camera which is closest to the newly introduced camera will give the best performance on the target camera and hence is more likely to adapt better than others. Second, they introduce a transitive inference algorithm to exploit information from best source camera to improve accuracy across other camera pairs. Extensive experiments on multiple benchmarks show that the proposed method significantly outperforms the state-of-the-art unsupervised alternatives while being extremely efficient to compute.</p></section></div></section><section id="Sec8" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading">Open Problems</h2><div class="content"><p id="Par19" class="Para">We now discuss two open research problems in re-identification, such as network-level knowledge transfer and learning in mobile networks.</p><section id="Sec9" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading">Knowledge Transfer Across Networks</h3><p id="Par20" class="Para">Above, we explained how it is possible to add a new target camera to an existing network of cameras using transfer learning with no additional supervision for the new camera. However, transfer learning across networks is still a largely under-addressed problem with many challenges. Given multiple existing source networks and a newly installed target network with limited labeled data, we first need to find the relevance/similarity of each source network, or parts thereof, in terms of amount of knowledge that it can transfer to a target network. Developing efficient statistical measures for finding relevance in a multi-camera network with significant changes in viewing angle, lighting, background clutter, and occlusion can be a very interesting future work. Furthermore, labeled data from source networks are often a subject of legal, technical, and contractual constraints between data owners and customers. Thus, existing transfer learning approaches may not be directly applicable in such scenarios where the source data is absent. The question we want to ask here is whether learned source models, instead of source data, can be used as a proxy for knowledge transfer across networks. Compared to the source data, the well-trained source model(s) are usually freely accessible in many applications and contain equivalent source knowledge as well. Knowledge distillation [<span class="CitationRef"><a href="#CR21">21</a></span>] along with attention transfer techniques can be adopted to transfer knowledge from a number of existing labeled networks to an unlabeled target network containing targets which never appeared in the source network.</p></section><section id="Sec10" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading">Learning in Mobile Camera Networks</h3><p id="Par21" class="Para">Despite the success of existing person re-identification works in static platforms, considering mobile cameras (e.g., network of robots) opens up exciting new research problems in terms of learning such data association models. It is not possible to learn transformation models between every possible pair of views in two mobile cameras due to the constantly changing nature of the videos being captured. Thus, in order to efficiently learn data association models, we need the data to represent the variety of scenarios that will be encountered by the mobile cameras. A semi-supervised pipeline that uses limited manual training data along with newly generated data through a generative adversarial network (GAN) could be a possibility. One initial approach could be to use the unlabeled samples produced by a Multi-view Generative Adversarial Network in conjunction with the labeled training data to learn view-invariant features in a mobile network. Moreover, apart from generating samples, we may need to evolve the learned models over time based on the observed features.</p></section></div></section></div><section class="Section1 RenderAsSection1" id="Bib1" tabindex="-1"><h2 class="Heading">References</h2><div class="content"><ol class="BibliographyWrapper"><li class="Citation"><div class="CitationNumber">1.</div><div class="CitationContent" id="CR1">Zheng L, Yang Y, Hauptmann AG (2016) Person re-identification: past, present and future. arXiv preprint:1610.02984<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Zheng%20L%2C%20Yang%20Y%2C%20Hauptmann%20AG%20%282016%29%20Person%20re-identification%3A%20past%2C%20present%20and%20future.%20arXiv%20preprint%3A1610.02984"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">2.</div><div class="CitationContent" id="CR2">Lisanti G, Masi I, Bagdanov AD, Del Bimbo A (2015) Person re-identification by iterative re-weighted sparse ranking. TPAMI 37:1629–1642<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1109/TPAMI.2014.2369055"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Person%20re-identification%20by%20iterative%20re-weighted%20sparse%20ranking&amp;author=G.%20Lisanti&amp;author=I.%20Masi&amp;author=AD.%20Bagdanov&amp;author=A.%20Bimbo&amp;journal=TPAMI&amp;volume=37&amp;pages=1629-1642&amp;publication_year=2015"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">3.</div><div class="CitationContent" id="CR3">Martinel N, Das A, Micheloni C, Roy-Chowdhury AK (2015) Re-identification in the function space of feature warps. TPAMI 37:1656–1669<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1109/TPAMI.2014.2377748"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Re-identification%20in%20the%20function%20space%20of%20feature%20warps&amp;author=N.%20Martinel&amp;author=A.%20Das&amp;author=C.%20Micheloni&amp;author=AK.%20Roy-Chowdhury&amp;journal=TPAMI&amp;volume=37&amp;pages=1656-1669&amp;publication_year=2015"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">4.</div><div class="CitationContent" id="CR4">Liao S, Hu Y, Zhu X, Li SZ (2015) Person re-identification by local maximal occurrence representation and metric learning. In: CVPR<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1109/CVPR.2015.7298832"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Person%20re-identification%20by%20local%20maximal%20occurrence%20representation%20and%20metric%20learning&amp;author=S.%20Liao&amp;author=Y.%20Hu&amp;author=X.%20Zhu&amp;author=SZ.%20Li&amp;publication_year=2015"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">5.</div><div class="CitationContent" id="CR5">Liao S, Li SZ (2015) Efficient psd constrained asymmetric metric learning for person re-identification. In: ICCV<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1109/ICCV.2015.420"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Efficient%20psd%20constrained%20asymmetric%20metric%20learning%20for%20person%20re-identification&amp;author=S.%20Liao&amp;author=SZ.%20Li&amp;publication_year=2015"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">6.</div><div class="CitationContent" id="CR6">Yi D, Lei Z, Liao S, Li SZ <em class="EmphasisTypeItalic ">et al</em> (2014) Deep metric learning for person re-identification. In: ICPR<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1109/ICPR.2014.16"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Deep%20metric%20learning%20for%20person%20re-identification&amp;author=D.%20Yi&amp;author=Z.%20Lei&amp;author=S.%20Liao&amp;author=SZ.%20Li&amp;publication_year=2014"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">7.</div><div class="CitationContent" id="CR7">Liu J, Zha ZJ, Tian Q, Liu D, Yao T, Ling Q, Mei T (2016) Multi-scale triplet CNN for person re-identification. In: Proceedings of the 2016 ACM on multimedia conference<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Liu%20J%2C%20Zha%20ZJ%2C%20Tian%20Q%2C%20Liu%20D%2C%20Yao%20T%2C%20Ling%20Q%2C%20Mei%20T%20%282016%29%20Multi-scale%20triplet%20CNN%20for%20person%20re-identification.%20In%3A%20Proceedings%20of%20the%202016%20ACM%20on%20multimedia%20conference"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">8.</div><div class="CitationContent" id="CR8">Das A, Panda R, Roy-Chowdhury A (2015) Active image pair selection for continuous person re-identification. In: ICIP<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1109/ICIP.2015.7351610"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Active%20image%20pair%20selection%20for%20continuous%20person%20re-identification&amp;author=A.%20Das&amp;author=R.%20Panda&amp;author=A.%20Roy-Chowdhury&amp;publication_year=2015"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">9.</div><div class="CitationContent" id="CR9">Martinel N, Das A, Micheloni C, Roy-Chowdhury AK (2016) Temporal model adaptation for person re-identification. In: ECCV<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1007/978-3-319-46493-0_52"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Temporal%20model%20adaptation%20for%20person%20re-identification&amp;author=N.%20Martinel&amp;author=A.%20Das&amp;author=C.%20Micheloni&amp;author=AK.%20Roy-Chowdhury&amp;publication_year=2016"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">10.</div><div class="CitationContent" id="CR10">Ma B, Su Y, Jurie F (2012) Local descriptors encoded by fisher vectors for person re-identification. In: ECCV<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1007/978-3-642-33863-2_41"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Local%20descriptors%20encoded%20by%20fisher%20vectors%20for%20person%20re-identification&amp;author=B.%20Ma&amp;author=Y.%20Su&amp;author=F.%20Jurie&amp;publication_year=2012"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">11.</div><div class="CitationContent" id="CR11">Zhao R, Ouyang W, Wang X (2013) Unsupervised salience learning for person re-identification. In: CVPR<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1109/CVPR.2013.460"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Unsupervised%20salience%20learning%20for%20person%20re-identification&amp;author=R.%20Zhao&amp;author=W.%20Ouyang&amp;author=X.%20Wang&amp;publication_year=2013"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">12.</div><div class="CitationContent" id="CR12">Kodirov E, Xiang T, Fu Z, Gong S (2016) Person re-identification by unsupervised∖ell _1 graph learning. In: ECCV<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1007/978-3-319-46448-0_11"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Person%20re-identification%20by%20unsupervised%E2%88%96ell%20_1%20graph%20learning&amp;author=E.%20Kodirov&amp;author=T.%20Xiang&amp;author=Z.%20Fu&amp;author=S.%20Gong&amp;publication_year=2016"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">13.</div><div class="CitationContent" id="CR13">Kulis B, Saenko K, Darrell T (2011) What you saw is not what you get: domain adaptation using asymmetric kernel transforms. In: CVPR<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=What%20you%20saw%20is%20not%20what%20you%20get%3A%20domain%20adaptation%20using%20asymmetric%20kernel%20transforms&amp;author=B.%20Kulis&amp;author=K.%20Saenko&amp;author=T.%20Darrell&amp;publication_year=2011"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">14.</div><div class="CitationContent" id="CR14">Patel VM, Gopalan R, Li R, Chellappa R (2015) Visual domain adaptation: a survey of recent advances. SPM 32:53–69<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Visual%20domain%20adaptation%3A%20a%20survey%20of%20recent%20advances&amp;author=VM.%20Patel&amp;author=R.%20Gopalan&amp;author=R.%20Li&amp;author=R.%20Chellappa&amp;journal=SPM&amp;volume=32&amp;pages=53-69&amp;publication_year=2015"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">15.</div><div class="CitationContent" id="CR15">Ma AJ, Li J, Yuen PC, Li P (2015) Cross-domain person reidentification using domain adaptation ranking SVMs. TIP 24(5):1599–613<span class="Occurrences"><span class="Occurrence OccurrenceAMSID"><a class="gtm-reference" data-reference-type="MathSciNet" target="_blank" rel="noopener" href="http://www.ams.org/mathscinet-getitem?mr=3325113"><span><span>MathSciNet</span></span></a></span><span class="Occurrence OccurrenceZLBID"><a class="gtm-reference" data-reference-type="MATH" target="_blank" rel="noopener" href="http://www.emis.de/MATH-item?1408.94457"><span><span>zbMATH</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Cross-domain%20person%20reidentification%20using%20domain%20adaptation%20ranking%20SVMs&amp;author=AJ.%20Ma&amp;author=J.%20Li&amp;author=PC.%20Yuen&amp;author=P.%20Li&amp;journal=TIP&amp;volume=24&amp;issue=5&amp;pages=1599-1613&amp;publication_year=2015"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">16.</div><div class="CitationContent" id="CR16">Chakraborty A, Das A, Roy-Chowdhury AK (2016) Network consistent data association. TPAMI 35:1622–1634<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Network%20consistent%20data%20association&amp;author=A.%20Chakraborty&amp;author=A.%20Das&amp;author=AK.%20Roy-Chowdhury&amp;journal=TPAMI&amp;volume=35&amp;pages=1622-1634&amp;publication_year=2016"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">17.</div><div class="CitationContent" id="CR17">Das A, Chakraborty A, Roy-Chowdhury AK (2014) Consistent re-identification in a camera network. In: ECCV<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1007/978-3-319-10605-2_22"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Consistent%20re-identification%20in%20a%20camera%20network&amp;author=A.%20Das&amp;author=A.%20Chakraborty&amp;author=AK.%20Roy-Chowdhury&amp;publication_year=2014"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">18.</div><div class="CitationContent" id="CR18">Roy S, Paul S, Young NE, Roy-Chowdhury AK (2018) Exploiting transitivity for learning person re-identification models on a budget. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 7064–7072<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Exploiting%20transitivity%20for%20learning%20person%20re-identification%20models%20on%20a%20budget&amp;author=S.%20Roy&amp;author=S.%20Paul&amp;author=NE.%20Young&amp;author=AK.%20Roy-Chowdhury&amp;publication_year=2018"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">19.</div><div class="CitationContent" id="CR19">Zheng L, Shen L, Tian L, Wang S, Wang J, Tian Q (2015) Scalable person re-identification: a benchmark. In: ICCV<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Scalable%20person%20re-identification%3A%20a%20benchmark&amp;author=L.%20Zheng&amp;author=L.%20Shen&amp;author=L.%20Tian&amp;author=S.%20Wang&amp;author=J.%20Wang&amp;author=Q.%20Tian&amp;publication_year=2015"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">20.</div><div class="CitationContent" id="CR20">Panda R, Bhuiyan A, Murino V, Roy-Chowdhury AK (2017) Unsupervised adaptive re-identification in open world dynamic camera networks. In: CVPR<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1109/CVPR.2017.151"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Unsupervised%20adaptive%20re-identification%20in%20open%20world%20dynamic%20camera%20networks&amp;author=R.%20Panda&amp;author=A.%20Bhuiyan&amp;author=V.%20Murino&amp;author=AK.%20Roy-Chowdhury&amp;publication_year=2017"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">21.</div><div class="CitationContent" id="CR21">Hinton G, Vinyals O, Dean J (2015) Distilling the knowledge in a neural network. arXiv preprint: 1503.02531<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Hinton%20G%2C%20Vinyals%20O%2C%20Dean%20J%20%282015%29%20Distilling%20the%20knowledge%20in%20a%20neural%20network.%20arXiv%20preprint%3A%201503.02531"><span><span>Google Scholar</span></span></a></span></span></div></li></ol></div></section><section class="Section1 RenderAsSection1"><h2 class="Heading" id="copyrightInformation">Copyright information</h2><div class="ArticleCopyright content"><div class="ChapterCopyright">© Springer Nature Switzerland AG 2020</div></div></section><section id="authorsandaffiliations" class="Section1 RenderAsSection1"><h2 class="Heading">Authors and Affiliations</h2><div class="content authors-affiliations u-interface"><ul class="test-contributor-names"><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors-affiliations__name">Rameswar Panda</span><ul class="authors-affiliations__indexes u-inline-list" data-role="AuthorsIndexes"><li data-affiliation="affiliation-1">1</li></ul></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors-affiliations__name">Amit K. Roy-Chowdhury</span><ul class="authors-affiliations__indexes u-inline-list" data-role="AuthorsIndexes"><li data-affiliation="affiliation-1">1</li></ul></li></ul><ol class="test-affiliations"><li class="affiliation" data-test="affiliation-1" data-affiliation-highlight="affiliation-1" itemscope="" itemtype="http://schema.org/Organization"><span class="affiliation__count">1.</span><span class="affiliation__item"><span itemprop="department" class="affiliation__department">Department of Electrical and Computer Engineering</span><span itemprop="name" class="affiliation__name">University of California</span><span itemprop="address" itemscope="" itemtype="http://schema.org/PostalAddress" class="affiliation__address"><span itemprop="addressRegion" class="affiliation__city">Riverside</span><span itemprop="addressCountry" class="affiliation__country">USA</span></span></span></li></ol></div></section><section class="Section1 RenderAsSection1"><h2 class="Heading" id="sectioneditors">Section editors and affiliations</h2><div class="content authors-affiliations u-interface"><ul class="test-contributor-names"><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors__name">Rama Chellappa</span><ul class="authors-affiliations__indexes u-inline-list" data-role="AuthorsIndexes"><li data-affiliation="affiliation-1">1</li></ul></li></ul><ol class="test-affiliations"><li class="affiliation" data-test="affiliation-1" data-affiliation-highlight="affiliation-1" itemscope="" itemtype="http://schema.org/Organization"><span class="affiliation__count">1.</span><span class="affiliation__item"><span itemprop="name" class="affiliation__name">University of Maryland</span><span itemprop="address" itemscope="" itemtype="http://schema.org/PostalAddress" class="affiliation__address"><span itemprop="addressRegion" class="affiliation__city">College Park</span><span itemprop="addressCountry" class="affiliation__country">United States</span></span></span></li></ol></div></section></div>
                            <aside class="section section--collapsible">
    <h2 id="howtocite" class="section__heading">How to cite</h2>
    <div class="section__content bibliographic-information citations">
        <dl class="citation-info u-highlight-target u-mb-16" id="citeas" tabindex="-1">
    <dt class="test-cite-heading">
        Cite this entry as:
    </dt>
    <dd id="citethis-text">Panda R., Roy-Chowdhury A.K. (2020) Person Re-identification: Current Approaches and Future Challenges. In: Ikeuchi K. (eds) Computer Vision. Springer, Cham</dd>
</dl>

        <ul class="citations__content citations__content--row" data-role="button-dropdown__content">
                <li>
                    <a href="//citation-needed.springer.com/v2/references/10.1007/978-3-030-03243-2_825-1?format&#x3D;refman&amp;flavour&#x3D;citation" title="Download this reference work entry&#39;s citation as a .RIS file" class="test-cite-link">
                        <span class="citations__extension" data-gtmlabel="RIS">
                            <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#004aa7"/></svg>
                            .RIS
                        </span>
                        <div class="citations__types">
                                <div>
                                    Papers
                                </div>
                                <div>
                                    Reference Manager
                                </div>
                                <div>
                                    RefWorks
                                </div>
                                <div>
                                    Zotero
                                </div>
                        </div>
                    </a>
                </li>
                <li>
                    <a href="//citation-needed.springer.com/v2/references/10.1007/978-3-030-03243-2_825-1?format&#x3D;endnote&amp;flavour&#x3D;citation" title="Download this reference work entry&#39;s citation as a .ENW file" class="test-cite-link">
                        <span class="citations__extension" data-gtmlabel="ENW">
                            <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#004aa7"/></svg>
                            .ENW
                        </span>
                        <div class="citations__types">
                                <div>
                                    EndNote
                                </div>
                        </div>
                    </a>
                </li>
                <li>
                    <a href="//citation-needed.springer.com/v2/references/10.1007/978-3-030-03243-2_825-1?format&#x3D;bibtex&amp;flavour&#x3D;citation" title="Download this reference work entry&#39;s citation as a .BIB file" class="test-cite-link">
                        <span class="citations__extension" data-gtmlabel="BIB">
                            <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#004aa7"/></svg>
                            .BIB
                        </span>
                        <div class="citations__types">
                                <div>
                                    BibTeX
                                </div>
                                <div>
                                    JabRef
                                </div>
                                <div>
                                    Mendeley
                                </div>
                        </div>
                    </a>
                </li>
        </ul>
    </div>
</aside>
                            <aside class="section section--collapsible" id="AboutThisContent">
    <h2 class="section__heading" id="aboutcontent">About this entry</h2>
    <div class="section__content bibliographic-information">
                <div id="crossMark" class="crossmark">
            <a data-crossmark="10.1007%2F978-3-030-03243-2_825-1" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007%2F978-3-030-03243-2_825-1" title="Verify currency and authenticity via CrossMark" data-track="click" data-track-action="Crossmark" data-track-label="">
                <span class="u-screenreader-only">CrossMark</span>
                <svg class="CrossMark" id="crossmark-icon" width="57" height="81">
                    <image width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="/springerlink-static/632953562/images/png/crossmark.png" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="/springerlink-static/632953562/images/svg/crossmark.svg"></image>
                </svg>
            </a>
        </div>

        <div class="crossmark__adjacent">
                <ul class="bibliographic-information__list bibliographic-information__list--inline">
            <li class="bibliographic-information__item">
                <span class="bibliographic-information__title">First Online</span>
                <span class="bibliographic-information__value u-overflow-wrap" data-test="bibliographic-first-online-date">10 January 2020</span>
            </li>
        <li class="bibliographic-information__item">
            <span class="bibliographic-information__title">DOI</span>
            <span class="bibliographic-information__value u-overflow-wrap" id="doi-url">https://doi.org/10.1007/978-3-030-03243-2</span>
        </li>
        <li class="bibliographic-information__item">
            <span class="bibliographic-information__title">Publisher Name</span>
            <span class="bibliographic-information__value" id="publisher-name">Springer, Cham</span>
        </li>
            <li class="bibliographic-information__item ">
                <span class="bibliographic-information__title">Online ISBN</span>
                <span class="bibliographic-information__value" id="bibliographic-electronic-isbn">978-3-030-03243-2</span>
            </li>
                <li class="bibliographic-information__item">
            <span class="bibliographic-information__title">eBook Packages</span>
                    <span class="bibliographic-information__value">Computer Science</span>
        </li>

    </ul>

            

            <ul class="bibliographic-information__list">
        <li class="bibliographic-information__item">
            <a id="reprintsandpermissions-link" target="_blank" rel="noopener" href="https://s100.copyright.com/AppDispatchServlet?publisherName&#x3D;SpringerNature&amp;orderBeanReset&#x3D;true&amp;orderSource&#x3D;SpringerLink&amp;copyright&#x3D;Springer+Nature+Switzerland+AG&amp;author&#x3D;Rameswar+Panda%2C+Amit+K.+Roy-Chowdhury&amp;contentID&#x3D;10.1007%2F978-3-030-03243-2_825-1&amp;endPage&#x3D;5&amp;publicationDate&#x3D;2020&amp;startPage&#x3D;1&amp;title&#x3D;Person+Re-identification%3A+Current+Approaches+and+Future+Challenges&amp;imprint&#x3D;Springer+Nature+Switzerland+AG&amp;publication&#x3D;eBook" title="Visit RightsLink for information about reusing this reference work entry" data-track="click" data-track-action="Reprints and Permissions" data-track-label="">Reprints and Permissions</a>
        </li>
</ul>



        </div>
        
            
    </div>
</aside>
                    </article>
                </div>

                    <aside id="main-aside" data-test="main-aside" class="main-aside--flex section section--non-collapsible" data-component="SpringerLink.LayoutHandler">



                        <div id="main-aside-content" class="c-tabs__content c-tabs--rwe__content u-js-hide u-js-show-two-col-medium">
                            <div id="springerlink-search" class="c-tabs__content-item c-tabs--rwe__content-item u-pt-16 u-pr-16">
                                <h2 class="u-show-micro u-hide-two-col-medium">Search book</h2>

<form class="autosuggest u-form-single-input u-pb-16" action="/search" method="get" role="search" data-component="SpringerLink.Autosuggest">
    <input type="hidden" name="facet-eisbn" value="978-3-030-03243-2"/>
    <input type="hidden" name="facet-content-type" value="ReferenceWorkEntry"/>
    <label for="springerlink-event-search">Search within book</label>
    <div class="u-relative">
        <input id="springerlink-event-search" name="query" type="text" autocomplete="off" value="" />
        <input class="u-hide-text" type="submit" value="Submit"
               title="Submit">
        <svg class="u-vertical-align-absolute" width="24" height="24" xmlns="http://www.w3.org/2000/svg" focusable="false" aria-hidden="true" role="presentation">
            <path d="M19.4804 18.4546c.283.2828.29.7492.0067 1.0325-.275.275-.7453.2806-1.0322-.0064l-3.0443-3.0442C14.2065 17.414 12.6717 18 11 18c-3.866 0-7-3.134-7-7s3.134-7 7-7 7 3.134 7 7c0 1.6717-.586 3.2066-1.5638 4.4103l3.0442 3.0443zM11 17c3.3137 0 6-2.6863 6-6s-2.6863-6-6-6-6 2.6863-6 6 2.6863 6 6 6z" fill="#666"/>
        </svg>
    </div>
</form>

<div class="u-hide u-js-show u-pt-16" data-role="autosuggest-output">
    <div class="autosuggest__message u-hide u-js-show" data-role="autosuggest-message">Type for suggestions</div>
</div>
                            </div>
                            <div id="toc" class="c-tabs__content-item c-tabs--rwe__content-item">
                                <h2 class="h2--rwe u-js-hide-two-col-medium">Table of contents</h2>


        <nav class="c-pagination c-pagination--rwe u-inline-block test-toc-pagination-top u-pt-16 u-pb-16 u-pr-16" role="navigation" aria-label="Pagination">

            <a href="#" class="test-pagination-previous c-pagination__previous u-hide" aria-hidden="true" id="js-previous" title="Previous" rel="prev">
                <span class="c-pagination__previous-icon">Previous</span>
            </a>

            <div class="c-pagination__info">
                Page&nbsp;&nbsp;
                <span class="test-pagenum u-js-hide" aria-current="true">1</span>

                <form id="js-page-number" action="/referenceworkentry/10.1007%2F978-3-030-03243-2_825-1#toc" method="GET" class="test-pageuri c-pagination__jump">
                    <label for="page-number-bottom" class="u-screenreader-only">Navigate to page number</label>
                    <input type="number" id="page-number-bottom" name="page" min="1" max="2" class="c-pagination__input" value="1"/>
                    <input class="c-pagination__submit u-screenreader-only" type="submit" value="Submit" aria-hidden="true">
                </form>

                &nbsp;&nbsp;of&nbsp;&nbsp;
                <span id="maxPageNumber" class="test-maxpagenum">2</span>
            </div>


            <a href="/referenceworkentry/10.1007%2F978-3-030-03243-2_825-1?page&#x3D;2#toc" id="js-next" class="test-pagination-next c-pagination__next "  title="Next" rel="next">
                <span class="c-pagination__next-icon">Next</span>
            </a>


        </nav>




<div class="rwe-toc-container test-referenceWorkToc u-pt-12 u-pb-16">
    <div data-component="SpringerLink.HighlightTocAnchorItems">
         <div class="sticky-sidebar-toc">
             <div class="js-contents">

                <div id="about-ref-work" class="rwe-toc-container "  data-test="test-about-ref-work ">
                    <h3 class="rwe-toc-container__heading u-sansSerif">About this reference work</h3>
                    <ol class="content-type-list content-type-list--aside">
        <li class="chapter-item content-type-list__item content-type-list__item--link-only introduction-toc-item">
            <div class="content-type-list__title">
                <a class="content-type-list__link u-sansSerif u-interface-link test-introduction-link" href=/referencework/10.1007/978-3-030-03243-2#introduction>Introduction</a>
            </div>
        </li>


        <li class="chapter-item content-type-list__item content-type-list__item--link-only editorsandaffiliations-toc-item">
            <div class="content-type-list__title">
                <a class="content-type-list__link u-sansSerif u-interface-link test-editorsandaffiliations-link" href=/referencework/10.1007/978-3-030-03243-2#editorsandaffiliations>Editors and affiliations</a>
            </div>
        </li>

        <li class="chapter-item content-type-list__item content-type-list__item--link-only bibliographic-info-toc-item">
            <div class="content-type-list__title">
                <a class="content-type-list__link u-sansSerif u-interface-link test-bibliographic-information-link" href=/referencework/10.1007/978-3-030-03243-2#bibliographic-info>Bibliographic information</a>
            </div>
        </li>
</ol>
                </div>

                <div class="rwe-toc-container u-border-top-gray">
                    <ol class="content-type-list content-type-list--aside" data-id="dynamic-toc-content">
            <li class="chapter-item content-type-list__item content-type-list__item--link-only">
                <div class="content-type-list__meta">
                        <div class="content-type-list__title">
                            <a class="content-type-list__link u-interface-link" href="/referenceworkentry/10.1007/978-3-030-03243-2_862-1">Autoencoder</a>
                        </div>
                </div>
            </li>
            <li class="chapter-item content-type-list__item content-type-list__item--link-only">
                <div class="content-type-list__meta">
                        <div class="content-type-list__title">
                            <a class="content-type-list__link u-interface-link" href="/referenceworkentry/10.1007/978-3-030-03243-2_454-1">Color Constancy</a>
                        </div>
                </div>
            </li>
            <li class="chapter-item content-type-list__item content-type-list__item--link-only">
                <div class="content-type-list__meta">
                        <div class="content-type-list__title">
                            <a class="content-type-list__link u-interface-link" href="/referenceworkentry/10.1007/978-3-030-03243-2_645-1">Digitization</a>
                        </div>
                </div>
            </li>
            <li class="chapter-item content-type-list__item content-type-list__item--link-only">
                <div class="content-type-list__meta">
                        <div class="content-type-list__title">
                            <a class="content-type-list__link u-interface-link" href="/referenceworkentry/10.1007/978-3-030-03243-2_819-1">Domain Adaptation Using Dictionaries</a>
                        </div>
                </div>
            </li>
            <li class="chapter-item content-type-list__item content-type-list__item--link-only">
                <div class="content-type-list__meta">
                        <div class="content-type-list__title">
                            <a class="content-type-list__link u-interface-link" href="/referenceworkentry/10.1007/978-3-030-03243-2_690-1">Dynamic Programming</a>
                        </div>
                </div>
            </li>
            <li class="chapter-item content-type-list__item content-type-list__item--link-only">
                <div class="content-type-list__meta">
                        <div class="content-type-list__title">
                            <a class="content-type-list__link u-interface-link" href="/referenceworkentry/10.1007/978-3-030-03243-2_711-1">Eigenspace Methods</a>
                        </div>
                </div>
            </li>
            <li class="chapter-item content-type-list__item content-type-list__item--link-only">
                <div class="content-type-list__meta">
                        <div class="content-type-list__title">
                            <a class="content-type-list__link u-interface-link" href="/referenceworkentry/10.1007/978-3-030-03243-2_692-1">Expectation-Maximization Algorithm</a>
                        </div>
                </div>
            </li>
            <li class="chapter-item content-type-list__item content-type-list__item--link-only">
                <div class="content-type-list__meta">
                        <div class="content-type-list__title">
                            <a class="content-type-list__link u-interface-link" href="/referenceworkentry/10.1007/978-3-030-03243-2_299-1">Feature Selection</a>
                        </div>
                </div>
            </li>
            <li class="chapter-item content-type-list__item content-type-list__item--link-only">
                <div class="content-type-list__meta">
                        <div class="content-type-list__title">
                            <a class="content-type-list__link u-interface-link" href="/referenceworkentry/10.1007/978-3-030-03243-2_657-1">Fisher-Rao Metric</a>
                        </div>
                </div>
            </li>
            <li class="chapter-item content-type-list__item content-type-list__item--link-only">
                <div class="content-type-list__meta">
                        <div class="content-type-list__title">
                            <a class="content-type-list__link u-interface-link" href="/referenceworkentry/10.1007/978-3-030-03243-2_817-1">Hashing for Face Search</a>
                        </div>
                </div>
            </li>
            <li class="chapter-item content-type-list__item content-type-list__item--link-only">
                <div class="content-type-list__meta">
                        <div class="content-type-list__title">
                            <a class="content-type-list__link u-interface-link" href="/referenceworkentry/10.1007/978-3-030-03243-2_233-1">Image Enhancement and Restoration: Traditional Approaches</a>
                        </div>
                </div>
            </li>
            <li class="chapter-item content-type-list__item content-type-list__item--link-only">
                <div class="content-type-list__meta">
                        <div class="content-type-list__title">
                            <a class="content-type-list__link u-interface-link" href="/referenceworkentry/10.1007/978-3-030-03243-2_13-1">Image Stitching</a>
                        </div>
                </div>
            </li>
            <li class="chapter-item content-type-list__item content-type-list__item--link-only">
                <div class="content-type-list__meta">
                        <div class="content-type-list__title">
                            <a class="content-type-list__link u-interface-link" href="/referenceworkentry/10.1007/978-3-030-03243-2_390-1">Line Drawing Labeling</a>
                        </div>
                </div>
            </li>
            <li class="chapter-item content-type-list__item content-type-list__item--link-only">
                <div class="content-type-list__meta">
                        <div class="content-type-list__title">
                            <a class="content-type-list__link u-interface-link" href="/referenceworkentry/10.1007/978-3-030-03243-2_648-1">Linear Programming</a>
                        </div>
                </div>
            </li>
            <li class="chapter-item content-type-list__item content-type-list__item--link-only">
                <div class="content-type-list__meta">
                        <div class="content-type-list__title">
                            <a class="content-type-list__link u-interface-link" href="/referenceworkentry/10.1007/978-3-030-03243-2_872-1">Monocular and Binocular People Tracking</a>
                        </div>
                </div>
            </li>
            <li class="chapter-item content-type-list__item content-type-list__item--link-only">
                <div class="content-type-list__meta">
                        <div class="content-type-list__title">
                            <a class="content-type-list__link u-interface-link" href="/referenceworkentry/10.1007/978-3-030-03243-2_827-1">Multidimensional Scaling</a>
                        </div>
                </div>
            </li>
            <li class="chapter-item content-type-list__item content-type-list__item--link-only content-type-list__item--current">
                <div class="content-type-list__meta">
                        <div class="content-type-list__title">
                            <a class="content-type-list__link u-interface-link" href="/referenceworkentry/10.1007/978-3-030-03243-2_825-1">Person Re-identification: Current Approaches and Future Challenges</a>
                        </div>
                </div>
            </li>
            <li class="chapter-item content-type-list__item content-type-list__item--link-only">
                <div class="content-type-list__meta">
                        <div class="content-type-list__title">
                            <a class="content-type-list__link u-interface-link" href="/referenceworkentry/10.1007/978-3-030-03243-2_649-1">Principal Component Analysis (PCA)</a>
                        </div>
                </div>
            </li>
            <li class="chapter-item content-type-list__item content-type-list__item--link-only">
                <div class="content-type-list__meta">
                        <div class="content-type-list__title">
                            <a class="content-type-list__link u-interface-link" href="/referenceworkentry/10.1007/978-3-030-03243-2_537-1">Reflectance Models</a>
                        </div>
                </div>
            </li>
            <li class="chapter-item content-type-list__item content-type-list__item--link-only">
                <div class="content-type-list__meta">
                        <div class="content-type-list__title">
                            <a class="content-type-list__link u-interface-link" href="/referenceworkentry/10.1007/978-3-030-03243-2_828-1">Regularization</a>
                        </div>
                </div>
            </li>
</ol>
                </div>
             </div>
         </div>
    </div>
</div>
                            </div>
                            <div id="download-book" class="c-tabs__content-item c-tabs--rwe__content-item">
                                
                            </div>
                        </div>
                    </aside>
            </div>
        </main>
        <div class="footer--flex">
                <footer class="footer u-interface">
        <div class="footer__aside-wrapper">
            <div class="footer__content footer__content--flex">
                <div class="footer__aside">
                    <p class="footer__strapline">Over 10 million scientific documents at your fingertips</p>
                                <div class="footer__edition" data-component="SV.EditionSwitcher">
                                    <h3 class="u-hide" data-role="button-dropdown__title" data-btn-text="Switch between Academic &#38; Corporate Edition">Switch Edition</h3>
                                    <ul data-role="button-dropdown__content">
                                        <li  class="selected"><a href="/siteEdition/link?previousUrl=/referenceworkentry/10.1007%2F978-3-030-03243-2_825-1&id=siteedition-academic-link" id="siteedition-academic-link">Academic Edition</a></li>
                                        <li ><a href="/siteEdition/rd?previousUrl=/referenceworkentry/10.1007%2F978-3-030-03243-2_825-1&id=siteedition-corporate-link" id="siteedition-corporate-link">Corporate Edition</a></li>
                                    </ul>
                                </div>
                </div>
            </div>
        </div>
        <div class="footer__content footer__content--flex">
            <ul class="footer__nav">
                <li>
                    <a href="/">Home</a>
                </li>
                <li>
                    <a href="/impressum">Impressum</a>
                </li>
                <li>
                    <a href="/termsandconditions">Legal information</a>
                </li>
                <li>
                    <a href="/privacystatement">Privacy statement</a>
                </li>
                <li>
                    <a href="/cookiepolicy">How we use cookies</a>
                </li>
                <li>
                    <a href="/accessibility">Accessibility</a>
                </li>
                <li>
                    <a id="contactus-footer-link" href="/contactus">Contact us</a>
                </li>
            </ul>
            <a class="parent-logo"
               target="_blank" rel="noopener"
               href="//www.springernature.com"
               title="Go to Springer Nature">
                <span class="u-screenreader-only">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12"
                           src="/springerlink-static/632953562/images/png/springernature.png"
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href="/springerlink-static/632953562/images/svg/springernature.svg">
                    </image>
                </svg>
            </a>

            <p class="footer__copyright">&copy; 2019 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>

                <p class="footer__user-access-info">
                    <span>Not logged in</span>
                    <span>SpringerReference Community (3000481153)</span>
                    <span>108.49.120.19</span>
                </p>
        </div>
    </footer>

        </div>
        <script type="text/javascript">
    (function() {
        var linkEl = document.querySelector('.js-ctm');
        var scriptsList = [];
        var polyfillFeatures = '';

        window.SpringerLink = window.SpringerLink || {};
        window.SpringerLink.staticLocation = '/springerlink-static/632953562';
        window.eventTrackerInstance = null;

        if (window.matchMedia && window.matchMedia(linkEl.media).matches) {
            (function(h){h.className = h.className.replace('no-js', 'js')})(document.documentElement);

            polyfillFeatures = 'default,fetch,Promise,Object.setPrototypeOf,Object.entries,Number.isInteger,MutationObserver,startsWith,Array.prototype.includes,Array.from,IntersectionObserver';

            scriptsList = [
                'https://cdn.polyfill.io/v2/polyfill.min.js?features=' + polyfillFeatures + '&flags=gated',
                window.SpringerLink.staticLocation + '/js/main.js'
            ];

            scriptsList.forEach(function(script) {
                var tag = document.createElement('script');
                tag.async = false;
                tag.src = script;

                document.body.appendChild(tag);
            });
        }
    })();
</script>

    <script>
    (function() {
        var linkEl = document.querySelector('.js-ctm');
        if (window.matchMedia && window.matchMedia(linkEl.media).matches) {
            var scriptMathJax = document.createElement('script');
            scriptMathJax.async = false;
            scriptMathJax.src = '/springerlink-static/632953562/js/mathJax.js';
            var s0 = document.getElementsByTagName('script')[0];
            s0.parentNode.insertBefore(scriptMathJax, s0);
        }
    })();
</script>




        
        <span id="chat-widget" class="u-hide"></span>
        
    </div>
</body>
</html>
