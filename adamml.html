
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
  body {
    font-family: "Titillium Web","HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:18px;
    margin-left: auto;
    margin-right: auto;
    width: 1100px;
  }

  h1 {
    font-weight:300;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  .link2 {
    text-decoration: none;
    display: inline;
    margin-right: 5px;
  }

  .fakelink {
    text-decoration: none;
    /* cursor: pointer; */
  }

  element.style {
    overflow: hidden;
    display: block;
  }
  .pre-white-space {
    white-space: pre;
  }
  .bibref {
    margin-top: 10px;
    margin-left: 10px;
    display: none;
    font-size: 14px;
    font-family: monospace;
  }

  img.rounded {
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
</style>
<script type="text/javascript" src="resources/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <head>
    <link rel="icon" type="image/png" href="resources/ucsd_logo.png">
    <title>AdaMML: Adaptive Multi-Modal Learning for Efficient Video Recognition</title>
    <meta property='og:title' content='Semi-Supervised Action Recognition with Temporal Contrastive Learning' />
    <meta property="og:description" content="Singh, Chakraborty, Varshney, Panda, Feris, Saenko, Das. Semi-Supervised Action Recognition with Temporal Contrastive Learning. In CVPR, 2021." />
    <meta property='og:url' content='https://cvir.github.io/TCL/' />
  </head>
  <body>
        <br>
        <center><span style="font-size:40px;font-weight:bold;color:#182B49">AdaMML: Adaptive Multi-Modal Learning for <br/> Efficient Video Recognition</span></center>

        <table align=center width=900px>
          <tr>
            <td align=center width=180px>
            <center><span style="font-size:20px"><a href="https://griffintaur.github.io/" target="_blank">Rameswar Panda</a><sup>1,&#8224</sup></span></center></td>
            <td align=center width=180px>
              <center><span style="font-size:20px"><a href="https://mitibmwatsonailab.mit.edu/people/richard-chen/" target="_blank"> Chun-Fu (Richard) Chen</a><sup>1,&#8224</sup></span></center></td>
            <td align=center width=180px>
                <center><span style="font-size:20px"><a href="https://mitibmwatsonailab.mit.edu/people/quanfu-fan/" target="_blank"> Quanfu Fan</a><sup>1</sup></span></center></td>
            <td align=center width=180px>
                <center><span style="font-size:20px"><a href="https://cs-people.bu.edu/sunxm/" target="_blank">Ximeng Sun</a><sup>2</sup></span></center></td>
            <tr/>
         </table>

         <table align=center width=600px>
            <tr>
              <td align=center width=150px>
                <center><span style="font-size:20px"><a href="http://ai.bu.edu/ksaenko.html" target="_blank">Kate Saenko</a><sup>1,2</sup></span></center></td>
          <td align=center width=150px>
            <center><span style="font-size:20px"><a href="http://olivalab.mit.edu/audeoliva.html" target="_blank">Aude Oliva</a><sup>1,3</sup></span></center></td>
        <td align=center width=150px>
            <center><span style="font-size:20px"><a href="http://rogerioferis.com/" target="_blank">Rogerio Feris</a><sup>1</sup></span></center></td>
        <tr/>
      </table>
        <center><span style="font-size:15px;color:#000000">&#8224: Equal Contribution</span></center>

        <table align=center width=800px>
          <tr>
            <td align=center width=150px><center><sup>1 </sup><span style="font-size:18px">MIT-IBM Watson AI Lab</span></center></td>
            <td align=center width=150px><center><sup>2 </sup><span style="font-size:18px">Boston University</span></center></td>
            <td align=center width=150px><center><sup>3 </sup><span style="font-size:18px">MIT</span></center></td>
          <tr/>
        </table> 
        <table align=center width=400px>
          <tr>
            <td align=center width=150px>
            <center><span style="font-size:24px"><a href="http://iccv2021.thecvf.com/" target="_blank">ICCV 2021</a></span></center></td>
          <tr/>
        </table>
        <table align=center width=200px>
            <tr><td width=200px>
              <center><a href="images/adamml_framework.png"><img src = "images/adamml_framework.png" width="900" height="300"></img></a><br></center>
            </td></tr>
        </table>

        <center id="abstract"><h1>Abstract</h1></center>
        Multi-modal learning, which focuses on utilizing various modalities to improve the performance of a model, is widely used in video recognition.
        While traditional multi-modal learning offers excellent recognition results, its computational expense limits its impact for many real-world applications.
        In this paper, we propose an adaptive multi-modal learning framework, called AdaMML, that selects on-the-fly the optimal modalities for each segment conditioned on the input for efficient video recognition.
        Specifically, given a video segment, a multi-modal policy network is used to decide what modalities should be used for processing by the recognition model, with the goal of improving both accuracy and efficiency.
        We efficiently train the policy network jointly with the recognition model using standard back-propagation. Extensive experiments on four challenging diverse datasets demonstrate that our proposed adaptive approach
        yields 35%-55% reduction in computation when compared to the traditional baseline that simply uses all the modalities irrespective of the input, while also achieving consistent improvements in accuracy over the
        state-of-the-art methods.        <br>
        <hr>

        <center id="results0"><h1>Qualitative Results</h1></center>
        <table align=center width=500px>
            <tr><td width=500px>
              <center><a href="images/adamml_qual.png"><img src = "images/adamml_qual.png" width="900" height="500"></img></a><br></center>
            </td></tr>
          </table>
          <caption width=500px align="center">Qualitative examples showing the effectiveness of <strong>AdaMML</strong> in selecting the right modalities per video segment (marked by green borders)</caption>
        <br>
        <hr>

        
        <center id="sourceCode"><h1>Paper & Code</h1></center>


        <table align=center width=900px>
            <tr></tr>
          <tr>
            <td >
        <a href="https://cvir.github.io/TCL/"><img class="paperpreview" src="images/adamml_framework.png" width="250px"/></a>
          </td>
          <td></td>
          <td width=700px > <span style="font-size:20px">
            Rameswar Panda*, Chun-Fu (Richard) Chen*, Quanfu Fan, Ximeng Sun, Kate Saenko, Aude Oliva, Rogerio Feris<br/>
              <a href="">
                  AdaMML: Adaptive Multi-Modal Learning for Efficient Video Recognition</a> <br/> <i>International Conference on Computer Vision (ICCV)</i>, 2021 <br/>
            [<a href="https://rpand002.github.io/data/ICCV_2021_adamml.pdf">PDF</a>]
            [<a href="https://rpand002.github.io/data/ICCV_2021_adamml_supp.pdf">Supp</a>]
            [<a href="">Code (Coming Soon)</a>]
<!--[<a href="">Video Presentation</a>]-->
<!--[<a href="">Poster</a>]-->

</span>
        </td>
        </tr>

      </table>

      <br>
      <hr>

      <br/>

    <br><br>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
</body>
</html>
