
<html><head><meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<title>Self-Taught Localization project page</title>
<style type="text/css" media="screen">
html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td {
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}

a {
  color: #1772d0;
  text-decoration:none;
}

a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
}

a.paper {
  font-weight: bold;
  font-size: 12pt;
}

b.paper {
  font-weight: bold;
  font-size: 12pt;
}

* {
  margin: 0pt;
  padding: 0pt;
}

body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 800px;
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 14px;
  background: #eee;
}

h2 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 18pt;
  font-weight: 700;
}

h3 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  font-weight: 700;
}

strong {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 13px;
}

ul {
  list-style: circle;
}

img {
  border: none;
}

li {
  padding-bottom: 0.5em;
  margin-left: 1.4em;
}

strong, b {
  font-weight:bold;
}

em, i {
  font-style:italic;
}

div.section {
  clear: both;
  margin-bottom: 1.5em;
  background: #eee;
}

div.spanner {
  clear: both;
}

div.paper {
  clear: both;
  margin-top: 0.5em;
  margin-bottom: 1em;
  border: 1px solid #ddd;
  background: #fff;
  padding: 1em 1em 1em 1em;
}

div.paper div {
  padding-left: 200px;
}

img.paper {
  margin-bottom: 0.5em;
  float: left;
  width: 170px;
}

div.dissert {
  clear: both;
  margin-top: 0.5em;
  margin-bottom: 1em;
  border: 1px solid #ddd;
  background: #fff;
  padding: 1em 1em 1em 1em;
}

div.dissert div {
  padding-left: 150px;
}

img.dissert {
  margin-bottom: 0.5em;
  float: left;
  width: 140px;
}

span.blurb {
  font-style:italic;
  display:block;
  margin-top:0.75em;
  margin-bottom:0.5em;
}

pre, code {
  font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
  margin: 1em 0;
  padding: 0;
}

div.paper pre {
  font-size: 0.9em;
}

</style>

<script type="text/javascript" async="" src="./page_files/ga.js"></script><script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-7953909-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>

<script type="text/javascript" src="./page_files/hidebib.js"></script>

<link href="./page_files/css" rel="stylesheet" type="text/css">
<!--<link href='http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300' rel='stylesheet' type='text/css'>-->
<!--<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>-->
<!--<link href='http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz' rel='stylesheet' type='text/css'>-->
<style id="style-1-cropbar-clipper">/* Copyright 2014 Evernote Corporation. All rights reserved. */
.en-markup-crop-options {
    top: 18px !important;
    left: 50% !important;
    margin-left: -100px !important;
    width: 200px !important;
    border: 2px rgba(255,255,255,.38) solid !important;
    border-radius: 4px !important;
}

.en-markup-crop-options div div:first-of-type {
    margin-left: 0px !important;
}
</style></head>

<body>

<div class="section">
<h2 id="STL">Self-taught Object Localization with Deep Networks</h2>

<div class="paper" id="Bazzani:wacv16">
  <img class="paper" title="WACV 2016" src="./page_files/STL.png">
  <div>
    <a class="paper" href="http://www.lorisbazzani.info/papers/proceedings/BazzanietalWACV16.pdf">Self-taught Object Localization with Deep Networks</a><br>
    <strong>L. Bazzani</strong>, A. Bergamo, D. Anguelov, L. Torresani<br>
    In IEEE Winter Conference on Applications of Computer Vision (WACV), 2016<br>

    <a href="http://www.lorisbazzani.info/STL.html"> STL code </a> / <a href="http://arxiv.org/abs/1409.3964"> arXiv </a> / <a shape="rect" href="javascript:togglebib('Bazzani:wacv16')" class="togglebib">bibtex</a>
    <pre xml:space="preserve" style="display: none;">
@conference{Bazzani:WACV16
  title     = {Self-taught Object Localization with Deep Networks},
  author    = {Bazzani, L. and Bergamo A. and Anguelov, D. and
               Torresani, L.},
  booktitle   = {In IEEE Winter Conference on Applications of Computer Vision (WACV)},
  year      = {2016}
}
  </pre>

    <span class="blurb">We leverage deep convolutional networks trained for whole-image recognition to localize objects in images without additional human supervision.</span>
  </div>
  <div class="spanner"></div>
</div>


<h3>Details</h3>
<div class="paper">
  This paper introduces self-taught object localization, a novel approach that leverages deep convolutional networks trained for whole-image recognition to localize objects in images without additional human supervision, i.e., without using any ground-truth bounding boxes for training. The key idea is to analyze the change in the recognition scores when artificially masking out different regions of the image. The masking out of a region that includes the object typically causes a significant drop in recognition score. This idea is embedded into an agglomerative clustering technique that generates self-taught localization hypotheses. Our object localization scheme outperforms existing proposal methods in both precision and recall for small number of subwindow proposals (e.g., on ILSVRC-2012 it produces a relative gain of 23.4% over the state-of-the-art for top-1 hypothesis). Furthermore, our experiments show that the annotations automatically-generated by our method can be used to train object detectors yielding recognition results remarkably close to those obtained by training on manually-annotated bounding boxes.
<br> <br>
<a href="http://github.com/lorisbaz/"><img class="aligncenter  wp-image-866" alt="STL_pic" src="./page_files/grayout_effect_crop.png" width="750"></a>
</div>


<div class="paper">
<center><h3><a href="https://github.com/lorisbaz/self-taught_localization">Download STL</a></h3> (link to github)</center> <br>
See the instructions in the README.md file.
</div>


</div>

<div style="clear:both;">
  <p align="right"><font size="2"><a href="http://lorisbazzani.info/"><< Go back to main page</a></font></p><br>
</div>

<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>




</body></html>
